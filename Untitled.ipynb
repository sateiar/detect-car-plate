{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images list is ['testcase\\\\cropped-lp1-20200715-101719651Z.jpg', 'testcase\\\\cropped-lp1-20200715-101722096Z.jpg', 'testcase\\\\cropped-lp1-20200715-101736686Z.jpg', 'testcase\\\\cropped-lp1-20200715-104933724Z.jpg', 'testcase\\\\cropped-lp1-20200715-104935131Z.jpg', 'testcase\\\\cropped-lp1-20200715-104941984Z.jpg', 'testcase\\\\cropped-lp1-20200715-140424901Z.jpg', 'testcase\\\\cropped-lp1-20200715-140444096Z.jpg', 'testcase\\\\cropped-lp1-20200715-140457486Z.jpg', 'testcase\\\\cropped-lp1-20200715-140458512Z.jpg']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "    for filename in os.listdir(folder):\n",
    "        image_paths.append(os.path.join(folder,filename))\n",
    "    print('images list is {}'.format (image_paths))\n",
    "    return image_paths\n",
    "### load images \n",
    "foldername= 'testcase'\n",
    "image_path_list=load_images_from_folder(foldername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "class ImageEntropy:\n",
    "    def __init__(self, file):\n",
    "        self.greyIm = file\n",
    "\n",
    "    def entropy(self, signal):\n",
    "        '''\n",
    "        function returns entropy of a signal\n",
    "        signal must be a 1-D numpy array\n",
    "        '''\n",
    "        lensig = signal.size\n",
    "        symset = list(set(signal))\n",
    "        numsym = len(symset)\n",
    "        propab = [np.size(signal[signal == i]) / (1.0 * lensig) for i in symset]\n",
    "        ent = np.sum([p * np.log2(1.0 / p) for p in propab])\n",
    "        return ent\n",
    "\n",
    "    def img_entro(self):\n",
    "        greyImg = np.array(self.greyIm)\n",
    "        region = greyImg.flatten()\n",
    "        result = self.entropy(region)\n",
    "        return result\n",
    "\n",
    "\n",
    "def remove_line(img):\n",
    "    result = img.copy()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh_img = cv2.adaptiveThreshold(gray,\n",
    "                                       255,\n",
    "                                       cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                       cv2.THRESH_BINARY_INV,\n",
    "                                       25,\n",
    "                                       15)\n",
    "    # Remove horizontal lines\n",
    "    horizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (80, 1))\n",
    "    remove_horizontal = cv2.morphologyEx(thresh_img,\n",
    "                                         cv2.MORPH_OPEN,\n",
    "                                         horizontal_kernel,\n",
    "                                         iterations=1)\n",
    "    cnts = cv2.findContours(remove_horizontal,\n",
    "                            cv2.RETR_EXTERNAL,\n",
    "                            cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    for c in cnts:\n",
    "        cv2.drawContours(result, [c], -1, (255, 255, 255), 7)\n",
    "    # Remove vertical lines\n",
    "    vertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 80))\n",
    "    remove_vertical = cv2.morphologyEx(thresh_img,\n",
    "                                       cv2.MORPH_OPEN,\n",
    "                                       vertical_kernel,\n",
    "                                       iterations=1)\n",
    "    cnts = cv2.findContours(remove_vertical,\n",
    "                            cv2.RETR_EXTERNAL,\n",
    "                            cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "    for c in cnts:\n",
    "        cv2.drawContours(result, [c], 0, (255, 255, 255), 7)\n",
    "\n",
    "    repair_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "    result_2 = 255 - cv2.morphologyEx(255 - result,\n",
    "                                      cv2.MORPH_CLOSE,\n",
    "                                      repair_kernel,\n",
    "                                      iterations=1)\n",
    "\n",
    "    return result_2\n",
    "\n",
    "def remove_text(img):\n",
    "    result = img - remove_line(img)\n",
    "    final_result = 255 - result\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import tesserocr\n",
    "from tesserocr import PyTessBaseAPI\n",
    "from PIL import Image\n",
    "from whitening import whiten\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def RGB2HEX(color):\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(color[0]), int(color[1]), int(color[2]))\n",
    "\n",
    "def detection(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    modified_image = cv2.resize(image, (60, 40), interpolation = cv2.INTER_AREA)\n",
    "    modified_image = modified_image.reshape(modified_image.shape[0]*modified_image.shape[1], 3)\n",
    "    number_of_colors = 3\n",
    "    clf = KMeans(n_clusters=number_of_colors)\n",
    "    labels = clf.fit_predict(modified_image)\n",
    "    counts = Counter(labels)\n",
    "\n",
    "    center_colors = clf.cluster_centers_\n",
    "    # We get ordered colors by iterating through the keys\n",
    "    ordered_colors = [center_colors[i] for i in counts.keys()]\n",
    "\n",
    "    hex_colors = [RGB2HEX(ordered_colors[i]) for i in counts.keys()]\n",
    "    rgb_colors = [ordered_colors[i] for i in counts.keys()]\n",
    "\n",
    "    flag = 'gray'\n",
    "    for texture in rgb_colors:\n",
    "        if texture.min() < 190:\n",
    "            flag='color'\n",
    "    return flag\n",
    "\n",
    "def binarization(gray):\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255,\n",
    "        cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY_INV, 25, 15)\n",
    "    return thresh\n",
    "\n",
    "\n",
    "def deblurin(thresh):\n",
    "    gray = cv2.medianBlur(thresh, 9)\n",
    "    return gray\n",
    "\n",
    "\n",
    "def sharping(gray):\n",
    "    gray = cv2.Canny(gray, 30, 200)\n",
    "    return gray\n",
    "\n",
    "\n",
    "def denoising(gray, result):\n",
    "    ret, thresh_img = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (4, 8))\n",
    "\n",
    "    morph_img = cv2.morphologyEx(thresh_img, cv2.MORPH_CLOSE, kernel)\n",
    "    thresh = cv2.adaptiveThreshold(morph_img.astype(np.uint8), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 9,\n",
    "                                   41)\n",
    "    # Repair image\n",
    "    repair_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 1))\n",
    "    final_result = 255 - cv2.morphologyEx(255 - result, cv2.MORPH_CLOSE, repair_kernel, iterations=1)\n",
    "    return final_result\n",
    "\n",
    "def ocr_preprocessing(path, output):\n",
    "    input_image = Image.open(path)\n",
    "    ###\n",
    "    print('load image ' + str(path))\n",
    "    head, tail = os.path.split(path)\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, (600, 400), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "\n",
    "#     img = cv2.Canny(img, 30, 200)\n",
    "\n",
    "    entro = ImageEntropy(remove_text(img)).img_entro()\n",
    "#     if entro > 0.3:\n",
    "#         img = remove_line(img)\n",
    "    # detect real color not real color\n",
    "    # img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    print('Image is converted from RGB to Grayscale space')\n",
    "\n",
    "#     flag = detection(img)\n",
    "#     if flag == 'gray':\n",
    "#     result = img.copy()\n",
    "#     ret, result = cv2.threshold(gray, 127, 250, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "    result = gray.copy()\n",
    "#     elif flag == 'color':\n",
    "    \n",
    "\n",
    "    thresh = binarization(gray)\n",
    "    print('binarzation is done ... ')\n",
    "\n",
    "    gray = deblurin(thresh)\n",
    "    # Smoothing the image\n",
    "    print('Debluring Filter is applied ... ')\n",
    "\n",
    "    #     # detect and sharpen the edge of characters for ocr\n",
    "\n",
    "    gray = sharping(gray)\n",
    "    print('Sharpping Filter is applied ... ')\n",
    "\n",
    "    final_result = denoising(gray, result)\n",
    "    print('Denoising is finished')\n",
    "    pathout = os.path.join(output, tail)\n",
    "    final_result = cv2.bilateralFilter(final_result, 13, 15, 15) \n",
    "    cv2.imwrite(pathout, final_result)\n",
    "    \n",
    "    return pathout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load image testcase\\cropped-lp1-20200715-101719651Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-101719651Z.jpg is :2%\n",
      "\n",
      "Time Process OCR : 0.15320396423339844\n",
      "done\n",
      "load image testcase\\cropped-lp1-20200715-101722096Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-101722096Z.jpg is :20\n",
      "\n",
      "Time Process OCR : 0.10477566719055176\n",
      "done\n",
      "load image testcase\\cropped-lp1-20200715-101736686Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-101736686Z.jpg is :ot 2}\n",
      "\n",
      "Time Process OCR : 0.12818503379821777\n",
      "done\n",
      "load image testcase\\cropped-lp1-20200715-104933724Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-104933724Z.jpg is :i 3E\n",
      "\n",
      "Time Process OCR : 0.13481783866882324\n",
      "done\n",
      "load image testcase\\cropped-lp1-20200715-104935131Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-104935131Z.jpg is :\n",
      "Time Process OCR : 0.012010335922241211\n",
      "done\n",
      "load image testcase\\cropped-lp1-20200715-104941984Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-104941984Z.jpg is :BYE\n",
      "\n",
      "Time Process OCR : 0.14945173263549805\n",
      "done\n",
      "load image testcase\\cropped-lp1-20200715-140424901Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-140424901Z.jpg is :-\n",
      "*\n",
      "-t 8 =\n",
      "ele i\n",
      "\n",
      "Time Process OCR : 0.22958087921142578\n",
      "done\n",
      "load image testcase\\cropped-lp1-20200715-140444096Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-140444096Z.jpg is :A\"\n",
      "\n",
      "Time Process OCR : 0.12753748893737793\n",
      "done\n",
      "load image testcase\\cropped-lp1-20200715-140457486Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-140457486Z.jpg is :i)\n",
      "\n",
      "Time Process OCR : 0.12942194938659668\n",
      "done\n",
      "load image testcase\\cropped-lp1-20200715-140458512Z.jpg\n",
      "Image is converted from RGB to Grayscale space\n",
      "binarzation is done ... \n",
      "Debluring Filter is applied ... \n",
      "Sharpping Filter is applied ... \n",
      "Denoising is finished\n",
      "OCR Process begin .....\n",
      "text image output\\cropped-lp1-20200715-140458512Z.jpg is :a)\n",
      "\n",
      "Time Process OCR : 0.1287989616394043\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from tesserocr import PyTessBaseAPI , RIL\n",
    "import tesserocr\n",
    "import time\n",
    "output_path = 'output'\n",
    "api = PyTessBaseAPI(lang='eng', psm=tesserocr.PSM.SINGLE_BLOCK)\n",
    "\n",
    "output_path_list = []\n",
    "for input_path in image_path_list :\n",
    "    filename=ocr_preprocessing(input_path, output_path)\n",
    "    try:\n",
    "        # api.SetImage(filename)\n",
    "        api.SetImageFile(filename)\n",
    "        print ( \"OCR Process begin .....\")\n",
    "        start_time = time.time()\n",
    "        b = api.GetUTF8Text()\n",
    "#         print(api.AllWords())\n",
    "        print('text image '+str(filename)+' is :'+str(b) )\n",
    "    #     blocks = b.splitlines()\n",
    "        boxes = api.GetComponentImages(RIL.TEXTLINE, True)\n",
    "#         print (a)\n",
    "#         print(api.GetUTF8Text())\n",
    "#         print (api.GetBoxText())\n",
    "    #     print(api.AllWords())\n",
    "    #     a = api.AllWords()\n",
    "    #     c=api.AdaptToWordStr()\n",
    "    #     print (api.AdaptToWordStr())\n",
    "    #     confident = api.AllWordConfidences()\n",
    "    #     confident = [x for x in confident if x > 50]\n",
    "        # confidence_all = api.AllWordConfidences()\n",
    "        # print(confident)\n",
    "    #     if len(confident)>0:\n",
    "    #         avg_confident=sum(confident)/len(confident)\n",
    "    #     else :\n",
    "    #         avg_confident = 0\n",
    "        end_time = time.time ()\n",
    "        print (\"Time Process OCR : \" + str (end_time - start_time))\n",
    "    finally:\n",
    "        print('done')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "brightness_4\n",
    "# specify path to the license plate images folder as shown below \n",
    "path_for_license_plates = os.getcwd() + \"/license-plates/**/*.jpg\"\n",
    "list_license_plates = [] \n",
    "predicted_license_plates = [] \n",
    "  \n",
    "for path_to_license_plate in glob.glob(path_for_license_plates, recursive = True): \n",
    "      \n",
    "    license_plate_file = path_to_license_plate.split(\"/\")[-1] \n",
    "    license_plate, _ = os.path.splitext(license_plate_file) \n",
    "    ''' \n",
    "    Here we append the actual license plate to a list \n",
    "    '''\n",
    "    list_license_plates.append(license_plate) \n",
    "      \n",
    "    ''' \n",
    "    Read each license plate image file using openCV \n",
    "    '''\n",
    "    img = cv2.imread(path_to_license_plate) \n",
    "      \n",
    "    ''' \n",
    "    We then pass each license plate image file \n",
    "    to the Tesseract OCR engine using the Python library  \n",
    "    wrapper for it. We get back predicted_result for  \n",
    "    license plate. We append the predicted_result in a \n",
    "    list and compare it with the original the license plate \n",
    "    '''\n",
    "    predicted_result = pytesseract.image_to_string(img, lang ='eng', config ='--oem 3 --psm 6 -c tessedit_char_whitelist = ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789') \n",
    "      \n",
    "    filter_predicted_result = \"\".join(predicted_result.split()).replace(\":\", \"\").replace(\"-\", \"\") \n",
    "    predicted_license_plates.append(filter_predicted_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETECTING PLATE . . .\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "import pytesseract as tess\n",
    "\n",
    "def preprocess(img):\n",
    "\tcv2.imshow(\"Input\",img)\n",
    "\timgBlurred = cv2.GaussianBlur(img, (5,5), 0)\n",
    "\tgray = cv2.cvtColor(imgBlurred, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\tsobelx = cv2.Sobel(gray,cv2.CV_8U,1,0,ksize=3)\n",
    "\t#cv2.imshow(\"Sobel\",sobelx)\n",
    "\t#cv2.waitKey(0)\n",
    "\tret2,threshold_img = cv2.threshold(sobelx,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\t#cv2.imshow(\"Threshold\",threshold_img)\n",
    "\t#cv2.waitKey(0)\n",
    "\treturn threshold_img\n",
    "\n",
    "def cleanPlate(plate):\n",
    "\tprint (\"CLEANING PLATE. . .\")\n",
    "\tgray = cv2.cvtColor(plate, cv2.COLOR_BGR2GRAY)\n",
    "\t#kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "\t#thresh= cv2.dilate(gray, kernel, iterations=1)\n",
    "\n",
    "\t_, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "\tim1,contours,hierarchy = cv2.findContours(thresh.copy(),cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "\tif contours:\n",
    "\t\tareas = [cv2.contourArea(c) for c in contours]\n",
    "\t\tmax_index = np.argmax(areas)\n",
    "\n",
    "\t\tmax_cnt = contours[max_index]\n",
    "\t\tmax_cntArea = areas[max_index]\n",
    "\t\tx,y,w,h = cv2.boundingRect(max_cnt)\n",
    "\n",
    "\t\tif not ratioCheck(max_cntArea,w,h):\n",
    "\t\t\treturn plate,None\n",
    "\n",
    "\t\tcleaned_final = thresh[y:y+h, x:x+w]\n",
    "\t\t#cv2.imshow(\"Function Test\",cleaned_final)\n",
    "\t\treturn cleaned_final,[x,y,w,h]\n",
    "\n",
    "\telse:\n",
    "\t\treturn plate,None\n",
    "\n",
    "\n",
    "def extract_contours(threshold_img):\n",
    "\telement = cv2.getStructuringElement(shape=cv2.MORPH_RECT, ksize=(17, 3))\n",
    "\tmorph_img_threshold = threshold_img.copy()\n",
    "\tcv2.morphologyEx(src=threshold_img, op=cv2.MORPH_CLOSE, kernel=element, dst=morph_img_threshold)\n",
    "\tcv2.imshow(\"Morphed\",morph_img_threshold)\n",
    "\tcv2.waitKey(0)\n",
    "\n",
    "\tcontours, hierarchy= cv2.findContours(morph_img_threshold,mode=cv2.RETR_EXTERNAL,method=cv2.CHAIN_APPROX_NONE)\n",
    "\treturn contours\n",
    "\n",
    "\n",
    "def ratioCheck(area, width, height):\n",
    "\tratio = float(width) / float(height)\n",
    "\tif ratio < 1:\n",
    "\t\tratio = 1 / ratio\n",
    "\n",
    "\taspect = 4.7272\n",
    "\tmin = 15*aspect*15  # minimum area\n",
    "\tmax = 125*aspect*125  # maximum area\n",
    "\n",
    "\trmin = 3\n",
    "\trmax = 6\n",
    "\n",
    "\tif (area < min or area > max) or (ratio < rmin or ratio > rmax):\n",
    "\t\treturn False\n",
    "\treturn True\n",
    "\n",
    "def isMaxWhite(plate):\n",
    "\tavg = np.mean(plate)\n",
    "\tif(avg>=115):\n",
    "\t\treturn True\n",
    "\telse:\n",
    " \t\treturn False\n",
    "\n",
    "def validateRotationAndRatio(rect):\n",
    "\t(x, y), (width, height), rect_angle = rect\n",
    "\n",
    "\tif(width>height):\n",
    "\t\tangle = -rect_angle\n",
    "\telse:\n",
    "\t\tangle = 90 + rect_angle\n",
    "\n",
    "\tif angle>15:\n",
    "\t \treturn False\n",
    "\n",
    "\tif height == 0 or width == 0:\n",
    "\t\treturn False\n",
    "\n",
    "\tarea = height*width\n",
    "\tif not ratioCheck(area,width,height):\n",
    "\t\treturn False\n",
    "\telse:\n",
    "\t\treturn True\n",
    "\n",
    "\n",
    "\n",
    "def cleanAndRead(img,contours):\n",
    "\t#count=0\n",
    "\tfor i,cnt in enumerate(contours):\n",
    "\t\tmin_rect = cv2.minAreaRect(cnt)\n",
    "\n",
    "\t\tif validateRotationAndRatio(min_rect):\n",
    "\n",
    "\t\t\tx,y,w,h = cv2.boundingRect(cnt)\n",
    "\t\t\tplate_img = img[y:y+h,x:x+w]\n",
    "\n",
    "\n",
    "\t\t\tif(isMaxWhite(plate_img)):\n",
    "\t\t\t\t#count+=1\n",
    "\t\t\t\tclean_plate, rect = cleanPlate(plate_img)\n",
    "\n",
    "\t\t\t\tif rect:\n",
    "\t\t\t\t\tx1,y1,w1,h1 = rect\n",
    "\t\t\t\t\tx,y,w,h = x+x1,y+y1,w1,h1\n",
    "\t\t\t\t\tcv2.imshow(\"Cleaned Plate\",clean_plate)\n",
    "\t\t\t\t\tcv2.waitKey(0)\n",
    "\t\t\t\t\tplate_im = Image.fromarray(clean_plate)\n",
    "\t\t\t\t\ttext = tess.image_to_string(plate_im, lang='eng')\n",
    "\t\t\t\t\tprint (\"Detected Text : \",text)\n",
    "\t\t\t\t\tfont = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\t\t\t\t\tbottomLeftCornerOfText = (x,y-10)\n",
    "\t\t\t\t\tfontScale = 1\n",
    "\t\t\t\t\tfontColor = (240,68,83)\n",
    "\t\t\t\t\tlineType = 2\n",
    "\n",
    "\t\t\t\t\timg = cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "\t\t\t\t\tcv2.putText(img,text,\n",
    " \t\t\t\t\t\tbottomLeftCornerOfText, \n",
    "\t\t\t\t\t\tfont, \n",
    "\t\t\t\t\t\tfontScale,\n",
    "\t\t\t\t\t\tfontColor,\n",
    "\t\t\t\t\t\tlineType)\n",
    "\n",
    "\t\t\t\t\tcv2.imshow(\"Detected Plate\",img)\n",
    "\t\t\t\t\tcv2.waitKey(0)\n",
    "\n",
    "\t#print \"No. of final cont : \" , count\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tprint (\"DETECTING PLATE . . .\")\n",
    "\n",
    "\t#img = cv2.imread(\"testData/Final.JPG\")\n",
    "\timg = cv2.imread(r\"D:\\tapway\\testcase\\cropped-lp1-20200715-104941984Z.jpg\")\n",
    "\n",
    "\tthreshold_img = preprocess(img)\n",
    "\tcontours= extract_contours(threshold_img)\n",
    "\n",
    "\t#if len(contours)!=0:\n",
    "\t\t#print len(contours) #Test\n",
    "\t\t# cv2.drawContours(img, contours, -1, (0,255,0), 1)\n",
    "\t\t# cv2.imshow(\"Contours\",img)\n",
    "\t\t# cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\tcleanAndRead(img,contours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "output_path = 'output'\n",
    "output_path_list = []\n",
    "\n",
    "for input_path in image_path_list :\n",
    "    try:\n",
    "        img = cv2.imread(input_path)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) \n",
    "        gray = cv2.bilateralFilter(gray, 13, 15, 15) \n",
    "\n",
    "        edged = cv2.Canny(gray, 30, 200) \n",
    "        contours = cv2.findContours(edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        contours = imutils.grab_contours(contours)\n",
    "        contours = sorted(contours, key = cv2.contourArea, reverse = True)[:10]\n",
    "        screenCnt = None\n",
    "\n",
    "        for c in contours:\n",
    "\n",
    "            peri = cv2.arcLength(c, True)\n",
    "            approx = cv2.approxPolyDP(c, 0.018 * peri, True)\n",
    "\n",
    "            if len(approx) == 4:\n",
    "                screenCnt = approx\n",
    "                break\n",
    "\n",
    "        if screenCnt is None:\n",
    "            detected = 0\n",
    "            print (\"No contour detected\")\n",
    "        else:\n",
    "             detected = 1\n",
    "\n",
    "        if detected == 1:\n",
    "            cv2.drawContours(img, [screenCnt], -1, (0, 0, 255), 3)\n",
    "\n",
    "        mask = np.zeros(gray.shape,np.uint8)\n",
    "        new_image = cv2.drawContours(mask,[screenCnt],0,255,-1,)\n",
    "        new_image = cv2.bitwise_and(img,img,mask=mask)\n",
    "\n",
    "        (x, y) = np.where(mask == 255)\n",
    "        (topx, topy) = (np.min(x), np.min(y))\n",
    "        (bottomx, bottomy) = (np.max(x), np.max(y))\n",
    "        Cropped = gray[topx:bottomx+1, topy:bottomy+1]\n",
    "    except:\n",
    "        print('no')\n",
    "\n",
    "#     output_path_list.append(ocr_preprocessing(input_path, output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tesserocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Process begin .....\n",
      "Time Process OCR : 0.03299760818481445\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#ORIGINAL\n",
    "import tesserocr\n",
    "import time\n",
    "# î‚° tesseract --help-psm\n",
    "# Page segmentation modes:\n",
    "#   0    Orientation and script detection (OSD) only.\n",
    "#   1    Automatic page segmentation with OSD.\n",
    "#   2    Automatic page segmentation, but no OSD, or OCR. (not implemented)\n",
    "#   3    Fully automatic page segmentation, but no OSD. (Default)\n",
    "#   4    Assume a single column of text of variable sizes.\n",
    "#   5    Assume a single uniform block of vertically aligned text.\n",
    "#   6    Assume a single uniform block of text.\n",
    "#   7    Treat the image as a single text line.\n",
    "#   8    Treat the image as a single word.\n",
    "#   9    Treat the image as a single word in a circle.\n",
    "#  10    Treat the image as a single character.\n",
    "#  11    Sparse text. Find as much text as possible in no particular order.\n",
    "#  12    Sparse text with OSD.\n",
    "#  13    Raw line. Treat the image as a single text line,\n",
    "#        bypassing hacks that are Tesseract-specific.\n",
    "# 217849 ajsd214h  \n",
    "from tesserocr import PyTessBaseAPI , RIL\n",
    "api = PyTessBaseAPI(lang='eng', psm=tesserocr.PSM.OSD_ONLY)\n",
    "# /Users/buildmachine/imagepython/tessdata-master\n",
    "# C:/imagepython/tessdata-master/tessdata-master\n",
    "\n",
    "filename=r\"D:\\testcase\\samplepdf_3_table_body.png\"\n",
    "try:\n",
    "    # api.SetImage(filename)\n",
    "    api.SetImageFile(filename)\n",
    "    print ( \"OCR Process begin .....\")\n",
    "    start_time = time.time()\n",
    "    b = api.GetUTF8Text()\n",
    "#     blocks = b.splitlines()\n",
    "    # boxes = api.GetComponentImages(RIL.TEXTLINE, True)\n",
    "    # print (a)\n",
    "#     print(api.GetUTF8Text())\n",
    "#     print (api.GetBoxText())\n",
    "#     print(api.AllWords())\n",
    "#     a = api.AllWords()\n",
    "#     c=api.AdaptToWordStr()\n",
    "#     print (api.AdaptToWordStr())\n",
    "#     confident = api.AllWordConfidences()\n",
    "#     confident = [x for x in confident if x > 50]\n",
    "    # confidence_all = api.AllWordConfidences()\n",
    "    # print(confident)\n",
    "#     if len(confident)>0:\n",
    "#         avg_confident=sum(confident)/len(confident)\n",
    "#     else :\n",
    "#         avg_confident = 0\n",
    "    end_time = time.time ()\n",
    "    print (\"Time Process OCR : \" + str (end_time - start_time))\n",
    "finally:\n",
    "    print('done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Process begin .....\n",
      "Time Process OCR : 0.49652647972106934\n",
      "done\n",
      "['SLUMP DIMENSION CT (Kalmd LOAD ~ (Nfmm2)', 'SAMPLE MARK (mm) (mm) (Kg/m?) (kN) (N/mm?) REMARKS', 'EDVS-BRIDGE3-SLAB-A 80 150x150x150 2210 663.7 295', 'EDVS-BRIDGE3-SLAB-B 80 150x150x150 2210 636.1 28.5']\n",
      "[96, 96, 96, 96, 96, 96, 96, 74, 96, 90, 96, 91, 96, 96, 95, 95, 89, 96, 53, 96, 94, 66]\n"
     ]
    }
   ],
   "source": [
    "a,b,c =ocr(r\"D:\\testcase\\samplepdf_3_table_body.png\")\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesserocr.RIL."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
